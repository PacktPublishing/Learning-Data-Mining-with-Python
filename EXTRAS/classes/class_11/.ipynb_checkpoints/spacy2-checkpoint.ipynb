{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "explicit-intake",
   "metadata": {},
   "source": [
    "# SPACY - continuation (English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eleven-county",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'download1' from 'spacy.cli' (/home/michal/.local/lib/python3.7/site-packages/spacy/cli/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7d21c68b5afe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcli\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdownload1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'es_core_news_sm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'download1' from 'spacy.cli' (/home/michal/.local/lib/python3.7/site-packages/spacy/cli/__init__.py)"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.cli import download\n",
    "print(download('es_core_news_sm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-cabin",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install spacy\n",
    "\n",
    "import spacy\n",
    "from spacy.cli import download\n",
    "print(download('en_core_web_sm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-cosmetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-thomas",
   "metadata": {},
   "source": [
    "# Sentence Detection\n",
    "\n",
    "Sentence Detection is the process of locating the start and end of sentences in a given text. This allows you to you divide a text into linguistically meaningful units. You’ll use these units when you’re processing your text to perform tasks such as part of speech tagging and entity extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-boating",
   "metadata": {},
   "outputs": [],
   "source": [
    "about_text = ('Gus Proto is a Python developer currently'\n",
    "               ' working for a London-based Fintech'\n",
    "               ' company. He is interested in learning'\n",
    "               ' Natural Language Processing.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "short-qatar",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_doc=nlp(about_text)\n",
    "list_sents=list(text_doc.sents)\n",
    "print(len(list_sents))\n",
    "print((list_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpful-surrey",
   "metadata": {},
   "source": [
    "# Tokenization in spaCy\n",
    "\n",
    "Tokenization is the next step after sentence detection. It allows you to identify the basic units in your text. These basic units are called tokens. Tokenization is useful because it breaks a text into meaningful units. These units are used for further analysis, like part of speech tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chief-georgia",
   "metadata": {},
   "outputs": [],
   "source": [
    "introduction_text = ('This tutorial is about Natural Language Processing in Spacy.')\n",
    "# we must create DOC object - a container\n",
    "introduction_doc = nlp(introduction_text)\n",
    "print ([token.text for token in introduction_doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-valentine",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'introduction.txt'\n",
    "introduction_file_text = open(file_name).read()\n",
    "introduction_doc = nlp(introduction_file_text)\n",
    "result=[token.text for token in introduction_doc]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annual-samuel",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in text_doc:\n",
    "    print(token, token.idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-signal",
   "metadata": {},
   "source": [
    "# Token attributes\n",
    "*    `token.idx` - returns a position of the token\n",
    "*    `is_alpha` - detects if the token consists of alphabetic characters or not.\n",
    "*    `is_punct` - detects if the token is a punctuation symbol or not.\n",
    "*    `is_space` - detects if the token is a space or not.\n",
    "*    `shape_` - prints out the shape of the word.\n",
    "*    `is_stop` - detects if the token is a stop word or not  \n",
    "\n",
    "https://spacy.io/api/token#attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-singer",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in text_doc:\n",
    "#     print (token, token.idx)\n",
    "#    print (token, token.is_lower)\n",
    "#    print (token, token.lower_)\n",
    "#    print (token, token.is_alpha)\n",
    "#     print (token, token.is_punct)\n",
    "#     print (token, token.is_space)    \n",
    "#     print (token, token.shape_)     \n",
    "     print (token, token.is_stop)       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neutral-innocent",
   "metadata": {},
   "source": [
    "# Stop Words\n",
    "\n",
    "Stop words are the most common words in a language. In the English language, some examples of stop words are the, are, but, and they. Most sentences need to contain stop words in order to be full sentences that make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-correlation",
   "metadata": {},
   "source": [
    "## english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-egypt",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "spacy_stopwords_en = spacy.lang.en.stop_words.STOP_WORDS\n",
    "print(len(spacy_stopwords_en))\n",
    "print()\n",
    "for stop_word in list(spacy_stopwords_en)[:10]:\n",
    "     print(stop_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tested-athletics",
   "metadata": {},
   "source": [
    "# spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preliminary-assault",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "spacy_stopwords_es = spacy.lang.es.stop_words.STOP_WORDS\n",
    "print(len(spacy_stopwords_es))\n",
    "print()\n",
    "for stop_word in list(spacy_stopwords_es)[:10]:\n",
    "     print(stop_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developing-illustration",
   "metadata": {},
   "source": [
    "## back to english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "square-folder",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in text_doc:\n",
    "     if not token.is_stop:\n",
    "         print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-broadway",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_spacy_stopwords_en = list(spacy_stopwords_en)\n",
    "for token in text_doc:\n",
    "#    if token not in list_spacy_stopwords_en[0:10]:\n",
    "    if token.text not in list_spacy_stopwords_en:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-bidding",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "\n",
    "Lemmatization is the process of reducing inflected forms of a word while still ensuring that the reduced form belongs to the language. This reduced form or root word is called a lemma.\n",
    "\n",
    "\n",
    "    comienza -> comenzar\n",
    "    comenzarán -> comenzar\n",
    "    clases -> clase\n",
    "\n",
    "# Stemming\n",
    "\n",
    "There is another way to reduce the number of tokens. This process consists of simply trimming the words to reduce them to a common base:\n",
    "\n",
    "    comienza -> comienz\n",
    "    comenzarán -> comenz\n",
    "    clases -> clas\n",
    "\n",
    "\n",
    "These two techniques are considered mutually exclusive, since you either apply one or apply the other, never both. But which is the most recommended?\n",
    "\n",
    "In general, lemmatization is always preferred, since it is a good compromise between reducing the amount of tokens and preserving a little more the original composition of these. Stemming being more aggressive tends to lead to a greater loss of information.\n",
    "\n",
    "In other words\n",
    "* Stemming drops the end of the word to retain a stable root. It is fast, but sometimes the results are difficult to interpret.\n",
    "* Lemmatization is smarter and takes into account the meaning of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aboriginal-senior",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "conference_help_text = ('Gus is helping organize a developer'\n",
    "     'conference on Applications of Natural Language'\n",
    "     ' Processing. He keeps organizing local Python meetups'\n",
    "     ' and several internal talks at his workplace.')\n",
    "conference_help_doc = nlp(conference_help_text)\n",
    "for token in conference_help_doc:\n",
    "     print (token, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scenic-updating",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"\"\"\n",
    "The crew of the USS Discovery discovered many discoveries.\n",
    "Discovering is what explorers do.\"\"\"\n",
    "conference_help_doc = nlp(string)\n",
    "for token in conference_help_doc:\n",
    "     print (token, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "departmental-baptist",
   "metadata": {},
   "source": [
    "# Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-jewelry",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "complete_text = ('Gus Proto is a Python developer currently '\n",
    "'working for a London-based Fintech company. He is'\n",
    "' interested in learning Natural Language Processing.'\n",
    "' There is a developer conference happening on 21 July'\n",
    "' 2019 in London. It is titled \"Applications of Natural'\n",
    "' Language Processing\". There is a helpline number '\n",
    "' available at +1-1234567891. Gus is helping organize it.'\n",
    "' He keeps organizing local Python meetups and several'\n",
    "' internal talks at his workplace. Gus is also presenting'\n",
    "' a talk. The talk will introduce the reader about \"Use'\n",
    "' cases of Natural Language Processing in Fintech\".'\n",
    "' Apart from his work, he is very passionate about music.'\n",
    "' Gus is learning to play the Piano. He has enrolled '\n",
    "' himself in the weekend batch of Great Piano Academy.'\n",
    "' Great Piano Academy is situated in Mayfair or the City'\n",
    "' of London and has world-class piano instructors.')\n",
    "\n",
    "complete_doc = nlp(complete_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "responsible-firmware",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [token.text for token in complete_doc if not token.is_stop and not token.is_punct and not token.is_space]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "textile-causing",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incident-findings",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = Counter(words)\n",
    "word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silent-flood",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twelve-quarterly",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beneficial-helping",
   "metadata": {},
   "outputs": [],
   "source": [
    "[word for (word, freq) in word_freq.items() if freq == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-perth",
   "metadata": {},
   "source": [
    "# Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "straight-glance",
   "metadata": {},
   "source": [
    "\n",
    "    Noun\n",
    "    Pronoun\n",
    "    Adjective\n",
    "    Verb\n",
    "    Adverb\n",
    "    Preposition\n",
    "    Conjunction\n",
    "    Interjection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "black-minneapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in complete_doc:\n",
    "     print (token, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portable-specification",
   "metadata": {},
   "source": [
    "## extracting adjectives only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revised-maldives",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in complete_doc:\n",
    "     if token.pos_==\"ADJ\":\n",
    "         print (token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appropriate-prince",
   "metadata": {},
   "source": [
    "# Named Entity Recognition\n",
    "Named Entity Recognition (NER) is the process of locating named entities in unstructured text and then classifying them into pre-defined categories, such as person names, organizations, locations, monetary values, percentages, time expressions, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attractive-kruger",
   "metadata": {},
   "outputs": [],
   "source": [
    "piano_class_text = (\"\"\"Great Piano Academy is situated in Mayfair or the City of \n",
    "                    London and has world-class piano instructors. 12 August 2022\"\"\")\n",
    "piano_class_doc = nlp(piano_class_text)\n",
    "print(piano_class_doc.ents)\n",
    "for ent in piano_class_doc.ents:\n",
    "     print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-canvas",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(piano_class_doc, style='ent',jupyter = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-feature",
   "metadata": {},
   "source": [
    "# Why would we want to \"reduce\" the amount of information through these transformations?\n",
    "\n",
    "The idea behind the elimination of stopwords, symbols, lemmatization or stemming lies in reducing the number of unique elements in our dataset, with the aim of increasing the performance of our algorithm in two ways:\n",
    "\n",
    "* Eliminating stopwords helps us eliminate common words that have little discriminative value between texts. Likewise, for many problems we do not need to know the tense in which a verb was written, or whether the word was \"corrupt\" or \"corruption\"; with base shapes the algorithm can “learn” a general idea.\n",
    "\n",
    "* This same idea can be applied for very rare tokens within our text… we could remove tokens that do not appear more than $X$ number of times, suspecting that they were perhaps misspellings or unimportant words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frozen-issue",
   "metadata": {},
   "source": [
    "# The order of cleaning the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constitutional-princeton",
   "metadata": {},
   "source": [
    "It is common that after tokenizing the text, the steps are applied in the order presented:\n",
    "\n",
    "* cleaning:\n",
    "  * Conversion to lowercase\n",
    "  * Removal special characters, RT @:, website links, multiple spaces, stopwords and punctuation symbols (replacing with space)\n",
    "* tokenization and saving tokens longer than 1 character\n",
    "* Lemmatization or stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tight-protein",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_texto(texto):\n",
    "    \n",
    "    nuevo_texto =texto\n",
    "    return nuevo_texto \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polyphonic-endorsement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "test = r\"RT @LondonEconomics: Hi guys the last meeting was a complete nonsense see the video here https://t.co/rnHPgyhx4Z @cienciadedatos #textmining\"\n",
    "print(test)\n",
    "print(limpiar_texto(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-metallic",
   "metadata": {},
   "source": [
    "# Word Vector Representation\n",
    "\n",
    "When we’re looking at words alone, it’s difficult for a machine to understand connections that a human would understand immediately. Engine and car, for example, have what might seem like an obvious connection (cars run using engines), but that link is not so obvious to a computer.\n",
    "\n",
    "Thankfully, there’s a way we can represent words that captures more of these sorts of connections. A word vector is a numeric representation of a word that commuicates its relationship to other words.\n",
    "\n",
    "Each word is interpreted as a unique and lenghty array of numbers. You can think of these numbers as being something like GPS coordinates. GPS coordinates consist of two numbers (latitude and longitude), and if we saw two sets GPS coordinates that were numberically close to each other (like 43,-70, and 44,-70), we would know that those two locations were relatively close together. Word vectors work similarly, although there are a lot more than two coordinates assigned to each word, so they’re much harder for a human to eyeball.\n",
    "\n",
    "Using spaCy‘s en_core_web_sm model, let’s take a look at the length of a vector for a single word, and what that vector looks like using .vector and .shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-hayes",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.cli import download\n",
    "print(download('en_core_web_sm'))\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virtual-killing",
   "metadata": {},
   "outputs": [],
   "source": [
    "mango = nlp(u'mango')\n",
    "print(mango.vector.shape)\n",
    "print(mango.vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "julian-daniel",
   "metadata": {},
   "source": [
    "There’s no way that a human could look at that array and identify it as meaning “mango,” but representing the word this way works well for machines, because it allows us to represent both the word’s meaning and its “proximity” to other similar words using the coordinates in the array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-cambridge",
   "metadata": {},
   "source": [
    "# Sentiment analysis\n",
    "\n",
    "Sentiment analysis, as fascinating as it is, is not without its flaws. Human language is nuanced and often far from straightforward. Machines might struggle to identify the emotions behind an individual piece of text despite their extensive grasp of past data. Some situations where sentiment analysis might fail are:\n",
    "\n",
    "    Sarcasm, jokes, irony. These things generally don’t follow a fixed set of rules, so they might not be correctly classified by sentiment analytics systems.\n",
    "    Nuance. Words can have multiple meanings and connotations, which are entirely subject to the context they occur in.\n",
    "    Multipolarity. When the given text is positive in some parts and negative in others.\n",
    "    Negation detection. It can be challenging for the machine because the function and the scope of the word ‘not’ in a sentence is not definite; moreover, suffixes and prefixes such as ‘non-,’ ‘dis-,’ ‘-less’ etc. can change the meaning of a text.\n",
    "\n",
    "\n",
    "## Sentiment analysis using Python NLP models\n",
    "####  using pretrained models\n",
    "* textblob\n",
    "\n",
    "* nlkt has vader\n",
    "https://neptune.ai/blog/sentiment-analysis-python-textblob-vs-vader-vs-flair\n",
    "\n",
    "* spacy has also build-in textblob\n",
    "https://spacy.io/universe/project/spacy-textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numeric-geology",
   "metadata": {},
   "source": [
    "# TextBlob\n",
    "https://textblob.readthedocs.io/en/dev/\n",
    "\n",
    "https://www.quora.com/What-is-polarity-and-subjectivity-in-sentiment-analysis?share=1\n",
    "\n",
    "TextBlob returns polarity and subjectivity of a sentence. Polarity lies between [-1,1], -1 defines a negative sentiment and 1 defines a positive sentiment. Negation words reverse the polarity. TextBlob has semantic labels that help with fine-grained analysis. For example — emoticons, exclamation mark, emojis, etc. Subjectivity lies between [0,1]. Subjectivity quantifies the amount of personal opinion and factual information contained in the text. The higher subjectivity means that the text contains personal opinion rather than factual information. TextBlob has one more parameter — intensity. TextBlob calculates subjectivity by looking at the ‘intensity’. Intensity determines if a word modifies the next word. For English, adverbs are used as modifiers (‘very good’).\n",
    "\n",
    "```\n",
    " Each word in the lexicon has scores for:\n",
    " 1)     polarity: negative vs. positive    (-1.0 => +1.0)\n",
    " 2) subjectivity: objective vs. subjective (+0.0 => +1.0)\n",
    " 3)    intensity: modifies next word?      (x0.5 => x2.0)\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electric-mapping",
   "metadata": {},
   "source": [
    "### TextBlob simple analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-dining",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can do a simple things like analysing a single word\n",
    "TextBlob(\"great\").sentiment\n",
    "## Sentiment(polarity=0.8, subjectivity=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitted-connectivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "TextBlob(\"not great\").sentiment\n",
    "## Sentiment(polarity=-0.4, subjectivity=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-compiler",
   "metadata": {},
   "outputs": [],
   "source": [
    "TextBlob(\"very great\").sentiment\n",
    "## Sentiment(polarity=1.0, subjectivity=0.9750000000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-antique",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = TextBlob(\"This movie is amazingly directed\")\n",
    "print(res.sentiment.polarity)\n",
    "print(res.sentiment.subjectivity)\n",
    "print(res.sentiment_assessments.assessments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smoking-remains",
   "metadata": {},
   "source": [
    "### TextBlob more advanced analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affected-submission",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "test = r\"RT @LondonEconomics: Hi guys I hated the last meeting, it was a complete shit see the video here https://t.co/rnHPgyhx4Z @cienciadedatos #textmining\"\n",
    "#test = r\"RT @LondonEconomics: Hi guys I loved the last meeting, was great, see the video here https://t.co/rnHPgyhx4Z @cienciadedatos #textmining\"\n",
    "\n",
    "text=limpiar_tokenizar(test)\n",
    "\n",
    "res = TextBlob(text)\n",
    "print(res.sentiment.polarity)\n",
    "print(res.sentiment.subjectivity)\n",
    "print(res.sentiment_assessments.assessments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rocky-classics",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe('spacytextblob')\n",
    "text = 'I had a really horrible day. It was the worst day ever! But every now and then I have a really good day that makes me happy.'\n",
    "doc = nlp(text)\n",
    "print(doc._.blob.polarity )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustained-complex",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc._.blob.polarity                            # Polarity: -0.125\n",
    "doc._.blob.subjectivity                        # Subjectivity: 0.9\n",
    "doc._.blob.sentiment_assessments.assessments   # Assessments: [(['really', 'horrible'], -1.0, 1.0, None), (['worst', '!'], -1.0, 1.0, None), (['really', 'good'], 0.7, 0.6000000000000001, None), (['happy'], 0.8, 1.0, None)]\n",
    "doc._.blob.ngrams()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polyphonic-racing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "data = [\"I love you\", \"I hate you\"]\n",
    "sentiment_pipeline(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naval-violation",
   "metadata": {},
   "source": [
    "# Sentiments in Spanish... ?\n",
    "https://huggingface.co/finiteautomata/beto-sentiment-analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-insured",
   "metadata": {},
   "source": [
    "#### sentiment-analysis-spanish\n",
    "https://pypi.org/project/sentiment-analysis-spanish/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preliminary-directory",
   "metadata": {},
   "source": [
    "The function sentiment(text) returns a number between 0 and 1. This is the probability of string variable text of being \"positive\". Low probabilities mean that the text is negative (numbers close to 0), high probabilities (numbers close to 1) mean that the text is positive. The space in between corespond to neutral texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-supervision",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentiment_analysis_spanish import sentiment_analysis\n",
    "sentiment = sentiment_analysis.SentimentAnalysisSpanish()\n",
    "print(sentiment.sentiment(\"me gusta la tombola es genial\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rocky-addiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = sentiment_analysis.SentimentAnalysisSpanish()\n",
    "print(sentiment.sentiment(\"me parece terrible esto que me estás diciendo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-brain",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
