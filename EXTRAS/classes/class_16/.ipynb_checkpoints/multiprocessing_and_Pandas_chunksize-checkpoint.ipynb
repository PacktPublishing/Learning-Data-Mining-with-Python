{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "278f2e15",
   "metadata": {},
   "source": [
    "\n",
    "# How to make things faster !!! (using your PC)\n",
    "* Multiprocessing\n",
    "\n",
    "Multiprocessing refers to the ability of a system to support more than one processor at the same time. Applications in a multiprocessing system are broken to smaller routines that run independently. The operating system allocates these threads to the processors improving performance of the system.\n",
    "* without multiprocessing CPUs are randomly assigned to tasks on your computer. For example one CPU runs your notebook, another one is processing Firefox (and even with this two tasks the CPU will be switching. CPU0 with CPU1 may be exchanging tasks )\n",
    "\n",
    "* very old concept: C,Fortran etc: If you really need speed for physics and simulations you have two options \n",
    "  * MPI - Message Passing Interface (MPI) - you can run your code on processors of different machines distributed over net - more complicated by great\n",
    "  * OpenMP - OpenMP (Open Multi-Processing) is an application programming interface - much simpler to implement but runs on local machine\n",
    "  \n",
    "  * all date for this class is here: https://mega.nz/folder/LAcGHJ4I#_uJ79tPCc4i5uWa0ps2itQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c094337e",
   "metadata": {},
   "source": [
    "#### Example Fortran OpenMP code:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa586fd",
   "metadata": {},
   "source": [
    "```\n",
    "program omp_par_do\n",
    "  implicit none\n",
    "\n",
    "  integer, parameter :: n = 100\n",
    "  real, dimension(n) :: dat, result\n",
    "  integer :: i\n",
    "\n",
    "  !$OMP PARALLEL DO\n",
    "  do i = 1, n\n",
    "     result(i) = my_function(dat(i))\n",
    "  end do\n",
    "  !$OMP END PARALLEL DO\n",
    "\n",
    "contains\n",
    "\n",
    "  function my_function(d) result(y)\n",
    "    real, intent(in) :: d\n",
    "    real :: y\n",
    "\n",
    "    ! do something complex with data to calculate y\n",
    "  end function my_function\n",
    "\n",
    "end program omp_par_do\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f02bb5",
   "metadata": {},
   "source": [
    "* multiprocessing in Python\n",
    "\n",
    "As CPU manufacturers start adding more and more cores to their processors, creating parallel code is a great way to improve performance. Python introduced multiprocessing module to let us write parallel code. To understand the main motivation of this module, we have to know some basics about parallel programming. After reading this article, we hope that, you would be able to gather some knowledge on this topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2093ce98",
   "metadata": {},
   "source": [
    "# Python `pool` vs `Process`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a05ff4",
   "metadata": {},
   "source": [
    "Crash course. Python multiprocessing is a separate topic and it may take time to learn it. So this is just a bried introduction with couple of examples:\n",
    "\n",
    "* The `pool` (like a swimming pool): allows you to do multiple jobs per process, which may make it easier to parallelize your program. If you have a million tasks to execute in parallel, you can create a Pool with a number of processes as many as CPU cores and then pass the list of the million tasks to pool.map. The pool will distribute those tasks to the worker processes(typically the same in number as available cores) and collects the return values in the form of a list and pass it to the parent process. Launching separate million processes would be much less practical (it would probably break your OS). `Pool` by itself distributes the tasks\n",
    "\n",
    "\n",
    "* The `Process`: On the other hand, if you have a small number of tasks to execute in parallel, and you only need each task done once, it may be perfectly reasonable to use a separate multiprocessing.process for each task, rather than setting up a Pool.  `Process` is manually controlled\n",
    "\n",
    "\n",
    "* How do they compare: the Pool allocates only executing processes in memory and the `process` allocates all the tasks in memory, so when the task number is small, we can use process class and when the task number is large, we can use the pool. In the case of large tasks, if we use a process then memory problems might occur, causing system disturbance. In the case of Pool, there is overhead in creating it. Hence with small task numbers, the performance is impacted when Pool is used.\n",
    "\n",
    "* multiprocessing can be used in cience for simulating differential equations, linear algebra (splitting arrays by parts) but also in data science.\n",
    "* as this is a data science course then lets bring examples related to the topic "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a728baa",
   "metadata": {},
   "source": [
    "Problem: Imagine you have 4 (400) large files that you want to analyze, they are around 200MB each and you want to find the sum of all the values from each file. You can do it\n",
    "* sequentially, load each file one by one find the sum then calculate the sum of sums "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1885d448",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9365be0",
   "metadata": {},
   "source": [
    "# This is slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ed163c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50000039990980.88, 50000039990980.88, 50000039990980.88, 50000039990980.88]\n",
      "CPU times: user 8.87 s, sys: 933 ms, total: 9.8 s\n",
      "Wall time: 10.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "\n",
    "def run(task,L):\n",
    "    df = pd.read_csv(task)\n",
    "    total_sum=df.sum().sum()\n",
    "    L.append(total_sum)\n",
    "\n",
    "L= []\n",
    "tasks = ['file1.csv','file1.csv','file1.csv','file1.csv']\n",
    "#tasks = ['file1.csv','file1.csv','file1.csv','file1.csv','file1.csv','file1.csv','file1.csv','file1.csv']\n",
    "for i in tasks:\n",
    "    run(i,L)\n",
    "print(L)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71cb864",
   "metadata": {},
   "source": [
    "* or you can do it using multiprocessing\n",
    "* you can split work by each cpu so they do the tasks simultaneously"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f112000",
   "metadata": {},
   "source": [
    "![](imgs/parallel_code.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11c6c32",
   "metadata": {},
   "source": [
    "# `multiprocessing` - checking number of CPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "610badab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f92e6c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cpu :  4\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of cpu : \", multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f8f23c",
   "metadata": {},
   "source": [
    "# Python  `map` function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384cae24",
   "metadata": {},
   "source": [
    "* first without map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1686c0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 8, 27, 64, 125]\n"
     ]
    }
   ],
   "source": [
    "org_list = [1, 2, 3, 4, 5]\n",
    "fin_list = []\n",
    "\n",
    "def cube(num):\n",
    "    return num**3\n",
    "\n",
    "for num in org_list:\n",
    "    fin_list.append(cube(num))\n",
    "\n",
    "print(fin_list) # [1, 8, 27, 64, 125]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb703d49",
   "metadata": {},
   "source": [
    "* now with `map`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b135ca90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 8, 27, 64, 125]\n"
     ]
    }
   ],
   "source": [
    "org_list = [1, 2, 3, 4, 5]\n",
    "\n",
    "def cube(num):\n",
    "    return num**3\n",
    "   \n",
    "fin_list = list(map(cube, org_list))\n",
    "print(fin_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b6c865",
   "metadata": {},
   "source": [
    "# This is fast (`pool - map` - pool CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17b449cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000039990980.8850000039990980.8850000039990980.8850000039990980.88\n",
      "\n",
      "\n",
      "\n",
      "CPU times: user 21 ms, sys: 28 ms, total: 49 ms\n",
      "Wall time: 5.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "\n",
    "def run(task):\n",
    "    df = pd.read_csv(task)\n",
    "    total_sum=df.sum().sum()\n",
    "    print(total_sum)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(processes=4)\n",
    "    tasks = ['file1.csv','file2.csv','file3.csv','file4.csv']\n",
    "    pool.map(run,tasks)\n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7004c9a3",
   "metadata": {},
   "source": [
    "* but we would like to save the results somewhere"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cfebca",
   "metadata": {},
   "source": [
    "# This is fast (`pool - apply_async` - pool CPU)\n",
    "* allows passing arguments so we can save the results in a special list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91a5cba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50000039990980.88, 50000039990980.88, 50000039990980.88, 50000039990980.88]\n",
      "CPU times: user 20 ms, sys: 37 ms, total: 57 ms\n",
      "Wall time: 5.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "\n",
    "def run(task,L):\n",
    "    df = pd.read_csv(task)\n",
    "    total_sum=df.sum().sum()\n",
    "    L2.append(total_sum)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    manager = mp.Manager()\n",
    "    L2 = manager.list()        \n",
    "    \n",
    "    pool = mp.Pool(processes=8)\n",
    "    tasks = ['file1.csv','file1.csv','file1.csv','file1.csv']\n",
    "    [pool.apply_async(run, args=[n,L2]) for n in tasks]\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    print(L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "20a0ad51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000159963923.53"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(list(L2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98297dc3",
   "metadata": {},
   "source": [
    "# This is also fast (`process` - individual CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7cf7b567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "[50000039990980.88, 50000039990980.88, 50000039990980.88, 50000039990980.88]\n",
      "CPU times: user 15 ms, sys: 25 ms, total: 40 ms\n",
      "Wall time: 5.25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Process\n",
    "import pandas as pd\n",
    "\n",
    "def run(task,L):\n",
    "    df = pd.read_csv(task)\n",
    "    total_sum=df.sum().sum()\n",
    "    L.append(total_sum)\n",
    "    return L\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    manager = mp.Manager()\n",
    "    L2 = manager.list()\n",
    "    \n",
    "    tasks = ['file1.csv','file1.csv','file1.csv','file1.csv']\n",
    "    my_process1 = Process(target=run, args=(tasks[0],L2))\n",
    "    my_process2 = Process(target=run, args=(tasks[1],L2))\n",
    "    my_process3 = Process(target=run, args=(tasks[2],L2))\n",
    "    my_process4 = Process(target=run, args=(tasks[3],L2))\n",
    "\n",
    "    my_process1.start()\n",
    "    my_process2.start()\n",
    "    my_process3.start()\n",
    "    my_process4.start()\n",
    "\n",
    "    my_process1.join()\n",
    "    my_process2.join()\n",
    "    my_process3.join()\n",
    "    my_process4.join()\n",
    "        \n",
    "\n",
    "    print (\"Done\")\n",
    "    print(L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cec522",
   "metadata": {},
   "source": [
    "# This is not so fast (`map - partial`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516b0fb7",
   "metadata": {},
   "source": [
    "# How to make things actually possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a98a658",
   "metadata": {},
   "source": [
    "Situation. I have a file that is 5GB big. I would not dare to open on my computer right now. So how can I analyze it on my personal PC with only 4-8GB RAM?\n",
    "* pandas has such a functionallity\n",
    "\n",
    "* You don’t have to read it all\n",
    "\n",
    "As an alternative to reading everything into memory, Pandas allows you to read data in chunks. In the case of CSV, we can load only some of the lines into memory at any given time.\n",
    "\n",
    "In particular, if we use the chunksize argument to pandas.read_csv, we get back an iterator over DataFrames, rather than one single DataFrame. Each DataFrame is the next 1000 lines of the CSV:\n",
    "\n",
    "* would be good to check how many lines has the file\n",
    "  * linux `$ wc -l filename\n",
    "            result:  #42448765`\n",
    "            42 Million records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97822b3c",
   "metadata": {},
   "source": [
    "https://mega.nz/folder/LAcGHJ4I#_uJ79tPCc4i5uWa0ps2itQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c25a4a7",
   "metadata": {},
   "source": [
    "# Fragment of the file - classic way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "236db074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_time</th>\n",
       "      <th>event_type</th>\n",
       "      <th>product_id</th>\n",
       "      <th>category_id</th>\n",
       "      <th>category_code</th>\n",
       "      <th>brand</th>\n",
       "      <th>price</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_session</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-10-01 00:00:00 UTC</td>\n",
       "      <td>view</td>\n",
       "      <td>44600062</td>\n",
       "      <td>2103807459595387724</td>\n",
       "      <td>NaN</td>\n",
       "      <td>shiseido</td>\n",
       "      <td>35.79</td>\n",
       "      <td>541312140</td>\n",
       "      <td>72d76fde-8bb3-4e00-8c23-a032dfed738c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-10-01 00:00:00 UTC</td>\n",
       "      <td>view</td>\n",
       "      <td>3900821</td>\n",
       "      <td>2053013552326770905</td>\n",
       "      <td>appliances.environment.water_heater</td>\n",
       "      <td>aqua</td>\n",
       "      <td>33.20</td>\n",
       "      <td>554748717</td>\n",
       "      <td>9333dfbd-b87a-4708-9857-6336556b0fcc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-10-01 00:00:01 UTC</td>\n",
       "      <td>view</td>\n",
       "      <td>17200506</td>\n",
       "      <td>2053013559792632471</td>\n",
       "      <td>furniture.living_room.sofa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>543.10</td>\n",
       "      <td>519107250</td>\n",
       "      <td>566511c2-e2e3-422b-b695-cf8e6e792ca8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-10-01 00:00:01 UTC</td>\n",
       "      <td>view</td>\n",
       "      <td>1307067</td>\n",
       "      <td>2053013558920217191</td>\n",
       "      <td>computers.notebook</td>\n",
       "      <td>lenovo</td>\n",
       "      <td>251.74</td>\n",
       "      <td>550050854</td>\n",
       "      <td>7c90fc70-0e80-4590-96f3-13c02c18c713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-10-01 00:00:04 UTC</td>\n",
       "      <td>view</td>\n",
       "      <td>1004237</td>\n",
       "      <td>2053013555631882655</td>\n",
       "      <td>electronics.smartphone</td>\n",
       "      <td>apple</td>\n",
       "      <td>1081.98</td>\n",
       "      <td>535871217</td>\n",
       "      <td>c6bd7419-2748-4c56-95b4-8cec9ff8b80d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                event_time event_type  product_id          category_id  \\\n",
       "0  2019-10-01 00:00:00 UTC       view    44600062  2103807459595387724   \n",
       "1  2019-10-01 00:00:00 UTC       view     3900821  2053013552326770905   \n",
       "2  2019-10-01 00:00:01 UTC       view    17200506  2053013559792632471   \n",
       "3  2019-10-01 00:00:01 UTC       view     1307067  2053013558920217191   \n",
       "4  2019-10-01 00:00:04 UTC       view     1004237  2053013555631882655   \n",
       "\n",
       "                         category_code     brand    price    user_id  \\\n",
       "0                                  NaN  shiseido    35.79  541312140   \n",
       "1  appliances.environment.water_heater      aqua    33.20  554748717   \n",
       "2           furniture.living_room.sofa       NaN   543.10  519107250   \n",
       "3                   computers.notebook    lenovo   251.74  550050854   \n",
       "4               electronics.smartphone     apple  1081.98  535871217   \n",
       "\n",
       "                           user_session  \n",
       "0  72d76fde-8bb3-4e00-8c23-a032dfed738c  \n",
       "1  9333dfbd-b87a-4708-9857-6336556b0fcc  \n",
       "2  566511c2-e2e3-422b-b695-cf8e6e792ca8  \n",
       "3  7c90fc70-0e80-4590-96f3-13c02c18c713  \n",
       "4  c6bd7419-2748-4c56-95b4-8cec9ff8b80d  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# read the large csv file with specified chunksize \n",
    "df_tiny = pd.read_csv('tiny.csv')\n",
    "df_tiny.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "93225a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3379.2599999999998"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tiny['price'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef529ea",
   "metadata": {},
   "source": [
    "# Fragment of the file - chunksize way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "17452edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# read the large csv file with specified chunksize \n",
    "df_chunk = pd.read_csv('tiny.csv', chunksize=1)\n",
    "# wc -l filename\n",
    "#42448765"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e1ee8db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.79\n",
      "33.2\n",
      "543.1\n",
      "251.74\n",
      "1081.98\n",
      "908.62\n",
      "380.96\n",
      "41.16\n",
      "102.71\n"
     ]
    }
   ],
   "source": [
    "chunk_list = [] \n",
    "for chunk in df_chunk:  \n",
    "    sum_chunk=chunk['price'].sum()\n",
    "    print(sum_chunk)\n",
    "    chunk_list.append(sum_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "567c6c02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3379.2599999999998"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(chunk_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7bceb4",
   "metadata": {},
   "source": [
    "# Entire file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "48d8e5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# read the large csv file with specified chunksize \n",
    "df_chunk = pd.read_csv('huge.csv', chunksize=1000000)\n",
    "# wc -l filename\n",
    "#42448765"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c6cde2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295982470.51\n",
      "297902352.25000006\n",
      "301375830.57000005\n",
      "297913145.99999994\n",
      "303370516.4999999\n",
      "295255042.6499999\n",
      "301581415.9699999\n",
      "301568463.49999976\n",
      "294448155.87\n",
      "271216902.56\n",
      "282985331.38999987\n",
      "285571944.2100001\n",
      "290204096.22\n",
      "279731917.1499999\n",
      "283156063.01000017\n",
      "279884552.8299999\n",
      "275989306.26\n",
      "293831560.08000016\n",
      "299158011.86\n",
      "295262903.45000017\n",
      "293112490.53999996\n",
      "293027094.5799999\n",
      "288816986.2000001\n",
      "290844328.4999999\n",
      "288347991.2799999\n",
      "288987846.3400001\n",
      "285289654.45\n",
      "291676301.64\n",
      "285775966.9099999\n",
      "286290020.6600001\n",
      "289038436.53000003\n",
      "288175044.58\n",
      "288575785.63\n",
      "287692333.57999986\n",
      "288871527.39\n",
      "289770061.83\n",
      "292651105.28000015\n",
      "291729216.77999985\n",
      "288960815.64000016\n",
      "288385370.20000005\n",
      "287038985.34999996\n",
      "291532345.50000006\n",
      "132900863.8\n"
     ]
    }
   ],
   "source": [
    "chunk_list = [] \n",
    "for chunk in df_chunk:  \n",
    "    sum_chunk=chunk['price'].sum()\n",
    "    print(sum_chunk)\n",
    "    chunk_list.append(sum_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "52807514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12323880556.029999"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(chunk_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
