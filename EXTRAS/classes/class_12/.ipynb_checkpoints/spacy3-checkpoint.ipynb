{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "maritime-lender",
   "metadata": {},
   "source": [
    "# In the last class we saw how to analyse the sentiments using pretrained models. Lets see how to train your own model\n",
    "\n",
    "* another quick crash course on machine learning using SKLEARN\n",
    "* your task: \n",
    "* Tweets about american arlines - based on the old tweets the airline wants to know if the tweet is positive or negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-wilson",
   "metadata": {},
   "source": [
    "### Text classification using machine learning\n",
    "\n",
    "* the power of this methods is that its purely statistical, can be sentences with errors, misspelled etc\n",
    "\n",
    "Now that we’ve looked at some of the cool things spaCy can do in general, let’s look at at a bigger real-world application of some of these natural language processing techniques: text classification. Quite often, we may find ourselves with a set of text data that we’d like to classify according to some parameters (perhaps the subject of each snippet, for example) and text classification is what will help us to do this.\n",
    "\n",
    "The diagram below illustrates the big-picture view of what we want to do when classifying text. First, we extract the features we want from our source text (and any tags or metadata it came with), and then we feed our cleaned data into a machine learning algorithm that do the classification for us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inappropriate-workstation",
   "metadata": {},
   "source": [
    "![](imgs/process.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-stylus",
   "metadata": {},
   "source": [
    "## Procedure\n",
    "* cleaning data (remove repeated rows etc, here we assume the dataset is clean)\n",
    "* understanding data (plots etc, skip this part for today only)\n",
    "* identifying columns that will constitute features and  data (features and labels)\n",
    "* make `features` list: list of senteces (.values or .to_numpy())\n",
    "* make `labels` list: list of labels (negative/positive) (.values or .to_numpy())\n",
    "* vectorize the sentences (bag of words or TfidfVectorizer)\n",
    "  https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "* split into X_train, X_test, y_train, y_test\n",
    "  https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "\n",
    "```  \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(processed_features_vec, labels, test_size=0.2, random_state=0)\n",
    "```\n",
    "\n",
    "* load classifier, train the model and check the accuracy (why? not regressor)\n",
    "  https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble\n",
    "\n",
    "```  \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "text_classifier = RandomForestClassifier(n_estimators=200, random_state=0)\n",
    "text_classifier.fit(X_train, y_train)\n",
    "predictions = text_classifier.predict(X_test)\n",
    "print(accuracy_score(y_test, predictions))  \n",
    "```\n",
    "*we will use the Random Forest algorithm, owing to its ability to act upon non-normalized data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "super-declaration",
   "metadata": {},
   "source": [
    "### some simple cleaning routine that you started during the last class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cardiovascular-significance",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3d6e1ab31126>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprocessed_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Remove all the special characters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprocessed_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\W'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'features' is not defined"
     ]
    }
   ],
   "source": [
    "processed_features = []\n",
    "\n",
    "for i in range(0, len(features)):\n",
    "    print\n",
    "    # Remove all the special characters\n",
    "    processed_feature = re.sub(r'\\W', ' ', str(features[i]))\n",
    "    processed_feature = processed_feature.lower()\n",
    "    ### your other claning lines\n",
    "\n",
    "    processed_features.append(processed_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "proved-appreciation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' virginamerica what dhepburn said '"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "unlimited-campus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14640"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-regression",
   "metadata": {},
   "source": [
    "# Machine feeds on numbers so we should convert the words into numbers\n",
    "\n",
    "* spacy can do it\n",
    "* but scikit-learn can do it better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "caring-blond",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96,)\n",
      "[-0.70611125 -1.4329455   0.24227941  0.6598132  -0.20285606 -0.3363567\n",
      " -1.4245116  -0.11146422 -0.56221646  0.3003068  -0.19000328 -0.08635545\n",
      "  1.3099948   1.379954    0.02685246  1.5109322  -0.733334    0.80945534\n",
      "  0.29014212 -0.2684864  -0.7413073  -0.7534003   1.52542    -0.61603916\n",
      "  0.3729881   0.31268534 -0.68583065 -0.75191927  0.58086497 -1.0955321\n",
      "  0.86638093 -1.9158285  -0.05129784 -0.20604798  0.2827754  -2.019856\n",
      " -0.0126412   0.3666329  -1.2550778   1.6548673  -0.85672385 -0.9216615\n",
      "  0.2952034   0.01230198 -0.42903078 -0.4966709  -0.25612807 -1.3058071\n",
      "  1.8100011   0.51152885  0.03403987  0.70565414  0.42585516 -0.8349808\n",
      "  0.5538808   0.57170147 -1.101404    0.33620203  0.07782254  0.5464119\n",
      " -0.06026481 -0.5734616   0.6843033  -1.0217375  -0.11573818 -0.93082213\n",
      " -0.85589534  0.5505712   1.3896189  -0.5574837   0.19777809  0.3153283\n",
      " -0.37644464  0.38533548  0.02513826 -0.293028   -0.23319107  0.8843169\n",
      "  0.61514205 -1.189681    1.3120099   0.49911803 -0.060248    0.5434863\n",
      " -0.26821092  1.2935734  -0.36863634  0.14278814 -0.09635624 -0.03814635\n",
      " -0.12483558 -0.31109205 -0.5967629  -0.41767922  0.87455094  0.20538166]\n"
     ]
    }
   ],
   "source": [
    "mango = nlp(u'mango')\n",
    "print(mango.vector.shape)\n",
    "print(mango.vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-helen",
   "metadata": {},
   "source": [
    "# Representing Text in Numeric Form\n",
    "\n",
    "Statistical algorithms use mathematics to train machine learning models. However, mathematics only work with numbers. To make statistical algorithms work with text, we first have to convert text to numbers. To do so, three main approaches exist i.e. Bag of Words, TF-IDF and Word2Vec. In this section, we will discuss the bag of words and TF-IDF scheme.\n",
    "Bag of Words\n",
    "\n",
    "### Bag of words scheme is the simplest way of converting text to numbers.\n",
    "\n",
    "For instance, you have three documents:\n",
    "\n",
    "    Doc1 = \"I like to play football\"\n",
    "    Doc2 = \"It is a good game\"\n",
    "    Doc3 = \"I prefer football over rugby\"\n",
    "\n",
    "In the bag of words approach the first step is to create a vocabulary of all the unique words. For the above three documents, our vocabulary will be:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-allergy",
   "metadata": {},
   "source": [
    "* bag of words:\n",
    "\n",
    "Vocab = [I, like, to, play, football, it, is, a, good, game, prefer, over, rugby]\n",
    "\n",
    "* For instance, for Doc1, the feature vector will look like this:\n",
    "\n",
    "[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extra-performance",
   "metadata": {},
   "source": [
    "#### Questions:\n",
    "* are words equally important?\n",
    "* if not which ones are more important (frequent or less frequent)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-cycling",
   "metadata": {},
   "source": [
    "#### TF-IDF\n",
    "\n",
    "In the bag of words approach, each word has the same weight. The idea behind the TF-IDF approach is that the words that occur less in all the documents and more in individual document contribute more towards classification.\n",
    "\n",
    "TF-IDF is a combination of two terms. Term frequency and Inverse Document frequency. They can be calculated as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closing-hammer",
   "metadata": {},
   "source": [
    "TF  = (Frequency of a word in the document)/(Total words in the document)\n",
    "\n",
    "IDF = Log((Total number of docs)/(Number of docs containing the word))\n",
    "\n",
    "#### Scikit-Learn has such tool that can vectorize the words:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "adequate-university",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "subjective-remark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "(4, 9)\n",
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]\n",
      " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
      "  0.28108867 0.         0.28108867]\n",
      " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
      "  0.26710379 0.51184851 0.26710379]\n",
      " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]:.2f\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\n",
    "     'This is the first document.',\n",
    "     'This document is the second document.',\n",
    "     'And this is the third one.',\n",
    "     'Is this the first document?',\n",
    " ]\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "#vectorizer.get_feature_names_out()\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.shape)\n",
    "print(f'{X.toarray()}:.2f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-conflict",
   "metadata": {},
   "source": [
    "### lets make it simpler by removing the stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "previous-connectivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "spacy_stopwords_en = spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "amateur-terrorism",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.        ]\n",
      " [0.78722298 0.61666846]\n",
      " [0.         0.        ]\n",
      " [1.         0.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.7/site-packages/sklearn/feature_extraction/text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\n",
    "     'This is the first document.',\n",
    "     'This document is the second document.',\n",
    "     'And this is the third one.',\n",
    "     'Is this the first document?',\n",
    " ]\n",
    "vectorizer = TfidfVectorizer(stop_words=spacy_stopwords_en)\n",
    "X = vectorizer.fit_transform(corpus).toarray()\n",
    "#vectorizer.get_feature_names_out()\n",
    "vectorizer.get_feature_names()\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "certified-aspect",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['document', 'second']\n",
      "(4, 2)\n",
      "[[1.         0.        ]\n",
      " [0.78722298 0.61666846]\n",
      " [0.         0.        ]\n",
      " [1.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#vectorizer.get_feature_names_out()\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.shape)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "representative-restriction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.62922751 0.77722116 0.         0.         0.        ]\n",
      " [0.78722298 0.         0.         0.61666846 0.        ]\n",
      " [0.         0.         0.70710678 0.         0.70710678]\n",
      " [0.62922751 0.77722116 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\n",
    "     'This is the first document.',\n",
    "     'This document is the second document.',\n",
    "     'And this is the third one.',\n",
    "     'Is this the first document?',\n",
    " ]\n",
    "vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'))\n",
    "X = vectorizer.fit_transform(corpus).toarray()\n",
    "#vectorizer.get_feature_names_out()\n",
    "vectorizer.get_feature_names()\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advised-praise",
   "metadata": {},
   "source": [
    "# Where doest the discrepancy come from?\n",
    "* nltk works better for english\n",
    "* spacy (aparently) works better for spanish \n",
    "\n",
    "* but do not worry, they are both super easy to work with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-lighting",
   "metadata": {},
   "source": [
    "# Apply all the steps to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source_url = \"https://raw.githubusercontent.com/mhemmg/datasets/master/nlp/airline_tweets.csv\"\n",
    "airline_tweets = pd.read_csv(data_source_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "synthetic-strike",
   "metadata": {},
   "source": [
    "#### extract features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-input",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "grand-gambling",
   "metadata": {},
   "source": [
    "#### clean features (sentences in the list of features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-boston",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "great-sweden",
   "metadata": {},
   "source": [
    "#### vectorize the sentences using `TfidfVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-science",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-oregon",
   "metadata": {},
   "source": [
    "#### Dividing Data into Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guilty-column",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "recovered-freight",
   "metadata": {},
   "source": [
    "#### Load the library (RandomForest) and train the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unexpected-break",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "surprised-timothy",
   "metadata": {},
   "source": [
    "#### try some sentence (remember to convert it first using `vectorizer.transform(test_features).toarray()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-beverage",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
