{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69dda7e7",
   "metadata": {},
   "source": [
    "### All the stuff is already in Github + the source files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f11f17",
   "metadata": {},
   "source": [
    "# For truly large files > 100G you cant do the procesing on your laptop\n",
    "* options: Hadoop and Spark (Pig) \n",
    "* the idea is the same as multiprocessing but on larger scale (clusters multiple CPU multiple drives, large RAM) \n",
    "* these things are obviously quite complex, but its important that you at least hear it and touch it\n",
    "* links: https://www.ibm.com/cloud/blog/hadoop-vs-spark https://phoenixnap.com/kb/hadoop-vs-spark "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a7c177",
   "metadata": {},
   "source": [
    "# Hadoop\n",
    "Apache Hadoop is a platform that got its start as a Yahoo project in 2006, which became a top-level Apache open-source project afterward. This framework handles large datasets in a distributed fashion. The Hadoop ecosystem is highly fault-tolerant and does not depend upon hardware to achieve high availability. This framework is designed with a vision to look for the failures at the application layer. It’s a general-purpose form of distributed processing that has several components: \n",
    "\n",
    "* Hadoop Distributed File System (HDFS): This stores files in a Hadoop-native format and parallelizes them across a cluster. It manages the storage of large sets of data across a Hadoop Cluster. Hadoop can handle both structured and unstructured data. \n",
    "\n",
    "* HAdoop has its own file system and storage system\n",
    "\n",
    "* MapReduce: It is the algorithm that actually processes the data in parallel to combine the pieces into the desired result. \n",
    "\n",
    "* Hadoop is built in Java, and accessible through many programming languages, for writing MapReduce code, including Python, through a Thrift client.  It’s available either open-source through the Apache distribution, or through vendors such as Cloudera (the largest Hadoop vendor by size and scope), MapR, or HortonWorks. \n",
    "* `MAP` - `REDUCE`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53665c16",
   "metadata": {},
   "source": [
    "![](imgs/hadoop_fs.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac2b43c",
   "metadata": {},
   "source": [
    "![](imgs/Map_Reduce.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cfcc1b",
   "metadata": {},
   "source": [
    "# Spark\n",
    "\n",
    "Apache Spark is an open-source tool. It is a newer project, initially developed in 2012, at the AMPLab at UC Berkeley. It is focused on processing data in parallel across a cluster, but the biggest difference is that it works in memory. It is designed to use RAM for caching and processing the data. Spark performs different types of big data workloads like:\n",
    "\n",
    "It does not have its own storage system like Hadoop has, so it requires a storage platform like HDFS.\n",
    "\n",
    "### RDD\n",
    "Spark uses Resilient Distributed Datasets (RDD) or Spark RDD which makes data processing faster. Also, this is the key feature of Spark that enables logical partitioning of data sets during computation. \n",
    "\n",
    "RDD stands for Resilient Distributed Dataset where each of the terms signifies its features.\n",
    "\n",
    "    Resilient: means it is fault tolerant by using RDD lineage graph (DAG). Hence, it makes it possible to do recomputation in case of node failure.\n",
    "    Distributed:  As datasets for Spark RDD resides in multiple nodes.\n",
    "    Dataset: records of data that you will work with.\n",
    "\n",
    "\n",
    "### Five main components of Apache Spark:\n",
    "\n",
    "* Apache Spark Core: It is responsible for functions like scheduling, input and output operations, task dispatching, etc.\n",
    "* Spark SQL: This is used to gather information about structured data and how the data is processed.\n",
    "* Spark Streaming: This component enables the processing of live data streams. \n",
    "* Machine Learning Library: The goal of this component is scalability and to make machine learning more accessible.\n",
    "* GraphX: This has a set of APIs that are used for facilitating graph analytics tasks.\n",
    "\n",
    "### Features of Spark\n",
    "\n",
    "* 1.  Fast Processing– Spark contains Resilient Distributed Dataset (RDD) which saves time in reading and writing operations, allowing it to run almost ten to one hundred times faster than Hadoop.\n",
    "\n",
    "* 2.  Flexibility– Apache Spark supports multiple languages and allows the developers to write applications in Java, Scala, R, or Python.\n",
    "\n",
    "* 3.  In-memory computing– Spark stores the data in the RAM of servers which allows quick access and in turn accelerates the speed of analytics.\n",
    "\n",
    "* 4.  Real-time processing– Spark is able to process real-time streaming data. Unlike Map-Reduce which processes only stored data.\n",
    "\n",
    "* 5.  Better analytics– Apache Spark consists of a rich set of SQL queries, machine learning algorithms, complex analytics, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a163103",
   "metadata": {},
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72d0f8e",
   "metadata": {},
   "source": [
    "![](imgs/Hadoop_Spark_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01a5360",
   "metadata": {},
   "source": [
    "# Spark is getting popular, for us is PySpark (you can even install it on your PC)\n",
    "* when it makes sense to use spark\n",
    "* if you have access to one\n",
    "* difference between Spark and multiprocessing:\n",
    "\n",
    "        * Spark has a much more powerfu and robust distributed computing framework than just multiprocessing.\n",
    "        * Spark provides better automatic distribution, partition and rescaling of parallel tasks. Scaling and scheduling spark code becomes an easier task than having to program your custom multiprocessing code to respond to larger amounts of data + computations. \n",
    "        \n",
    "* there is also \"Dask\" but only if you want to learn another library "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac15df2",
   "metadata": {},
   "source": [
    "![](imgs/Pandas_Dask_PySpark.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c11fa7",
   "metadata": {},
   "source": [
    "Spark is a system for cluster computing. When compared to other cluster computing systems (such as Hadoop), it is faster. It has Python, Scala, and Java high-level APIs. In Spark, writing parallel jobs is simple. Spark is the most active Apache project at the moment, processing a large number of datasets. Spark is written in Scala and provides API in Python, Scala, Java, and R. In Spark, DataFrames are distributed data collections that are organized into rows and columns. Each column in a DataFrame is given a name and a type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bc288a",
   "metadata": {},
   "source": [
    "# Features of Spark\n",
    "* Spark supports MAP-REDUCE\n",
    "* PySpark DataFrame is immutable (cannot be changed once created), fault-tolerant and Transformations are Lazy evaluation (they are not executed until actions are called). \n",
    "* PySpark DataFrame’s are distributed in the cluster (meaning the data in PySpark DataFrame’s are stored in different machines in a cluster) and any operations in PySpark executes in parallel on all machines.\n",
    "* PySpark can be executed on different cores and machines, unavailable in Pandas.\n",
    "* PySpark supports SQL queries to run transformations. All you need to do is create a Table/View from the PySpark DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79018b4f",
   "metadata": {},
   "source": [
    "## You can install pyspark on your computer but it can be tedious (let me know if it worked for you)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df85a74",
   "metadata": {},
   "source": [
    "## Comparing Pandas and PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a47d5345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "2464fbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [{'fruit': 'apple', 'cost': 67.89, 'city': 'sao paulo'},\n",
    "        {'fruit': 'mango', 'cost': 87.67, 'city': 'brasilia'},\n",
    "        {'fruit': 'apple', 'cost': 64.76, 'city': 'araraquara'},\n",
    "        {'fruit': 'banana','cost': 87.00, 'city': 'bertioga'},\n",
    "        {'fruit': 'guava', 'cost': 69.56, 'city': 'brasilia'},\n",
    "        {'fruit': 'mango', 'cost': 234.67, 'city': 'sao paulo'},\n",
    "        {'fruit': 'apple', 'cost': 143.00, 'city': 'brasilia'},\n",
    "        {'fruit': 'mango', 'cost': 49.0, 'city': 'bertioga'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "9fd9f81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Pandas=pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0d57db63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fruit</th>\n",
       "      <th>cost</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apple</td>\n",
       "      <td>67.89</td>\n",
       "      <td>sao paulo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mango</td>\n",
       "      <td>87.67</td>\n",
       "      <td>brasilia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>apple</td>\n",
       "      <td>64.76</td>\n",
       "      <td>araraquara</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>banana</td>\n",
       "      <td>87.00</td>\n",
       "      <td>bertioga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>guava</td>\n",
       "      <td>69.56</td>\n",
       "      <td>brasilia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fruit   cost        city\n",
       "0   apple  67.89   sao paulo\n",
       "1   mango  87.67    brasilia\n",
       "2   apple  64.76  araraquara\n",
       "3  banana  87.00    bertioga\n",
       "4   guava  69.56    brasilia"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Pandas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "32be3cca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "803.55"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Pandas['cost'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "6db655fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fruit</th>\n",
       "      <th>cost</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>city</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>araraquara</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bertioga</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brasilia</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sao paulo</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            fruit  cost\n",
       "city                   \n",
       "araraquara      1     1\n",
       "bertioga        2     2\n",
       "brasilia        3     3\n",
       "sao paulo       2     2"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Pandas.groupby('city').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "592a0380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+\n",
      "|      city|  cost| fruit|\n",
      "+----------+------+------+\n",
      "| sao paulo| 67.89| apple|\n",
      "|  brasilia| 87.67| mango|\n",
      "|araraquara| 64.76| apple|\n",
      "|  bertioga|  87.0|banana|\n",
      "|  brasilia| 69.56| guava|\n",
      "| sao paulo|234.67| mango|\n",
      "|  brasilia| 143.0| apple|\n",
      "|  bertioga|  49.0| mango|\n",
      "+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import the pyspark module\n",
    "import pyspark\n",
    "\n",
    "# import the sparksession from pyspark.sql module\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# instantiate - create sparksession and then give the app name\n",
    "spark = SparkSession.builder.appName('statistics_globe').getOrCreate()\n",
    "\n",
    "# creating a dataframe from the given list of dictionary\n",
    "df_Spark = spark.createDataFrame(data)\n",
    "\n",
    "# display the final dataframe\n",
    "df_Spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0515ef1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(sum(cost)=803.55)]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, mean, col, max\n",
    "df_Spark.select(sum('cost')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2feddeeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(avg(cost)=100.44375)]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Spark.select(mean('cost')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a03c9237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(cost)=234.67)]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Spark.select(max('cost')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b02f271a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      city|count|\n",
      "+----------+-----+\n",
      "|  bertioga|    2|\n",
      "|  brasilia|    3|\n",
      "| sao paulo|    2|\n",
      "|araraquara|    1|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_Spark.groupBy('city').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "bb947796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions: 4\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Partitions: \" + str(df_Spark.rdd.getNumPartitions()) )\n",
    "print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "96260a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- cost: double (nullable = true)\n",
      " |-- fruit: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_Spark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee024462",
   "metadata": {},
   "source": [
    "* PySpark supports SQL queries to run transformations. All you need to do is create a `Table/View` from the PySpark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "184c5a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Spark.createOrReplaceTempView(\"Prices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f0b6bce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+\n",
      "|     city|  cost| fruit|\n",
      "+---------+------+------+\n",
      "| brasilia| 87.67| mango|\n",
      "| bertioga|  87.0|banana|\n",
      "|sao paulo|234.67| mango|\n",
      "| brasilia| 143.0| apple|\n",
      "+---------+------+------+\n",
      "\n",
      "+----------+---------+\n",
      "|mean(cost)|max(cost)|\n",
      "+----------+---------+\n",
      "| 100.44375|   234.67|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from Prices where cost > 70\").show()\n",
    "spark.sql(\"select mean(cost), max(cost) from India\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527bbca4",
   "metadata": {},
   "source": [
    "# PySpark and large files - wikipedia views per second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f3e7dd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A reference to our tab-separated-file\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('statistics_globe').getOrCreate()\n",
    "csvFile = \"pageviews_by_second.tsv\"\n",
    "\n",
    "tempDF = (spark.read           # The DataFrameReader\n",
    "   .option('header', 'true')   # adding header\n",
    "   .option(\"sep\", \"\\t\")        # Use tab delimiter (default is comma-separator)\n",
    "   .csv(csvFile)               # Creates a DataFrame from CSV after reading in the file\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d7c0571d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+--------+\n",
      "|          timestamp|   site|requests|\n",
      "+-------------------+-------+--------+\n",
      "|2015-03-16T00:09:55| mobile|    1595|\n",
      "|2015-03-16T00:10:39| mobile|    1544|\n",
      "|2015-03-16T00:19:39|desktop|    2460|\n",
      "|2015-03-16T00:38:11|desktop|    2237|\n",
      "|2015-03-16T00:42:40| mobile|    1656|\n",
      "|2015-03-16T00:52:24|desktop|    2452|\n",
      "|2015-03-16T00:54:16| mobile|    1654|\n",
      "|2015-03-16T01:18:11| mobile|    1720|\n",
      "|2015-03-16T01:30:32|desktop|    2288|\n",
      "|2015-03-16T01:32:24| mobile|    1609|\n",
      "|2015-03-16T01:42:08|desktop|    2341|\n",
      "|2015-03-16T01:45:53| mobile|    1704|\n",
      "|2015-03-16T01:55:37|desktop|    2554|\n",
      "|2015-03-16T01:57:29| mobile|    1825|\n",
      "|2015-03-16T02:03:16|desktop|    2492|\n",
      "|2015-03-16T02:10:32| mobile|    1667|\n",
      "|2015-03-16T02:16:45|desktop|    2452|\n",
      "|2015-03-16T02:19:32|desktop|    2412|\n",
      "|2015-03-16T02:20:16|desktop|    2350|\n",
      "|2015-03-16T02:22:08| mobile|    1802|\n",
      "+-------------------+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tempDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a05dd2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|   site|  count|\n",
      "+-------+-------+\n",
      "|desktop|3600000|\n",
      "| mobile|3600000|\n",
      "+-------+-------+\n",
      "\n",
      "Read without chunks:  5.104434251785278 seconds\n"
     ]
    }
   ],
   "source": [
    "s_time = time.time()\n",
    "tempDF.groupBy('site').count().show()\n",
    "e_time = time.time()\n",
    "print(\"Read without chunks: \", (e_time-s_time), \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87aa48be",
   "metadata": {},
   "source": [
    "# PySpark - Huge files (just for taste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "3d6a9b9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: file:/home/michal/MEGASyncWork/Universidad_de_Antioquia/CARGAS/2022-1/Mineria_de_datos/Learning-Data-Mining-with-Python/EXTRAS/classes/class_17/huge.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-149-1afda98a4626>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'header'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'true'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sep\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# Use tab delimiter (default is comma-separator)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m    \u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsvFile\u001b[0m\u001b[0;34m)\u001b[0m               \u001b[0;31m# Creates a DataFrame from CSV after reading in the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m )\n",
      "\u001b[0;32m/usr/lib64/python3.7/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/home/michal/MEGASyncWork/Universidad_de_Antioquia/CARGAS/2022-1/Mineria_de_datos/Learning-Data-Mining-with-Python/EXTRAS/classes/class_17/huge.csv"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('statistics_globe').getOrCreate()\n",
    "csvFile = \"huge.csv\"\n",
    "\n",
    "tempDF = (spark.read           # The DataFrameReader\n",
    "   .option('header', 'true')\n",
    "   .option(\"sep\", \",\")         # Use tab delimiter (default is comma-separator)\n",
    "   .csv(csvFile)               # Creates a DataFrame from CSV after reading in the file\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea3b06f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+\n",
      "|          event_time|event_type|product_id|        category_id|       category_code|   brand|  price|  user_id|        user_session|\n",
      "+--------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+\n",
      "|2019-10-01 00:00:...|      view|  44600062|2103807459595387724|                null|shiseido|  35.79|541312140|72d76fde-8bb3-4e0...|\n",
      "|2019-10-01 00:00:...|      view|   3900821|2053013552326770905|appliances.enviro...|    aqua|  33.20|554748717|9333dfbd-b87a-470...|\n",
      "|2019-10-01 00:00:...|      view|  17200506|2053013559792632471|furniture.living_...|    null| 543.10|519107250|566511c2-e2e3-422...|\n",
      "|2019-10-01 00:00:...|      view|   1307067|2053013558920217191|  computers.notebook|  lenovo| 251.74|550050854|7c90fc70-0e80-459...|\n",
      "|2019-10-01 00:00:...|      view|   1004237|2053013555631882655|electronics.smart...|   apple|1081.98|535871217|c6bd7419-2748-4c5...|\n",
      "|2019-10-01 00:00:...|      view|   1480613|2053013561092866779|   computers.desktop|  pulser| 908.62|512742880|0d0d91c2-c9c2-4e8...|\n",
      "|2019-10-01 00:00:...|      view|  17300353|2053013553853497655|                null|   creed| 380.96|555447699|4fe811e9-91de-46d...|\n",
      "|2019-10-01 00:00:...|      view|  31500053|2053013558031024687|                null|luminarc|  41.16|550978835|6280d577-25c8-414...|\n",
      "|2019-10-01 00:00:...|      view|  28719074|2053013565480109009|  apparel.shoes.keds|   baden| 102.71|520571932|ac1cd4e5-a3ce-422...|\n",
      "|2019-10-01 00:00:...|      view|   1004545|2053013555631882655|electronics.smart...|  huawei| 566.01|537918940|406c46ed-90a4-478...|\n",
      "|2019-10-01 00:00:...|      view|   2900536|2053013554776244595|appliances.kitche...|elenberg|  51.46|555158050|b5bdd0b3-4ca2-4c5...|\n",
      "|2019-10-01 00:00:...|      view|   1005011|2053013555631882655|electronics.smart...| samsung| 900.64|530282093|50a293fb-5940-41b...|\n",
      "|2019-10-01 00:00:...|      view|   3900746|2053013552326770905|appliances.enviro...|   haier| 102.38|555444559|98b88fa0-d8fa-4b9...|\n",
      "|2019-10-01 00:00:...|      view|  44600062|2103807459595387724|                null|shiseido|  35.79|541312140|72d76fde-8bb3-4e0...|\n",
      "|2019-10-01 00:00:...|      view|  13500240|2053013557099889147|furniture.bedroom...|     brw|  93.18|555446365|7f0062d8-ead0-4e0...|\n",
      "|2019-10-01 00:00:...|      view|  23100006|2053013561638126333|                null|    null| 357.79|513642368|17566c27-0a8f-450...|\n",
      "|2019-10-01 00:00:...|      view|   1801995|2053013554415534427|electronics.video.tv|   haier| 193.03|537192226|e3151795-c355-4ef...|\n",
      "|2019-10-01 00:00:...|      view|  10900029|2053013555069845885|appliances.kitche...|   bosch|  58.95|519528062|901b9e3c-3f8f-414...|\n",
      "|2019-10-01 00:00:...|      view|   1306631|2053013558920217191|  computers.notebook|      hp| 580.89|550050854|7c90fc70-0e80-459...|\n",
      "|2019-10-01 00:00:...|      view|   1005135|2053013555631882655|electronics.smart...|   apple|1747.79|535871217|c6bd7419-2748-4c5...|\n",
      "+--------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tempDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "906f07d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(sum(price)=12323880556.028587)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "tempDF.select(sum('price')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c293ab17",
   "metadata": {},
   "source": [
    "# Spark/PySpark on Microsoft Azure (DataBricks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22947f47",
   "metadata": {},
   "source": [
    "### Databricks\n",
    "\n",
    "Deploy an Azure Databricks workspace\n",
    "1. Open the Azure portal\n",
    "2. Click Create a Resource in the top left\n",
    "3. Search for \"Databricks\"\n",
    "4. Select Azure Databricks\n",
    "5. On the Azure Databricks page select Create\n",
    "    \n",
    "6. Provide the required values to create your Azure Databricks workspace:  \n",
    "    Subscription: Choose the Azure subscription in which to deploy the workspace.  \n",
    "    esource Group: Use Create new and provide a name for the new resource group.  \n",
    "    Location: Select a location near you for deployment. \n",
    "    For the list of regions that are supported by Azure Databricks, see Azure services available by region. (US EAST 3)\n",
    "    Workspace Name: Provide a unique name for your workspace.  \n",
    "    Pricing Tier: Trial (Premium - 14 days Free DBUs).   \n",
    "    You must select this option when creating your workspace or you will be charged.   \n",
    "     The workspace will suspend automatically after 14 days. When the trial is over you can convert the workspace to Premium but then you will be charged for your usage.\n",
    "7. Select Review + Create.\n",
    "8. Select Create.\n",
    "\n",
    "The workspace creation takes a few minutes. During workspace creation, the Submitting deployment for Azure\n",
    "Databricks tile appears on the right side of the portal. You might need to scroll right on your dashboard to view the\n",
    "tile. There's also a progress bar displayed near the top of the screen. You can watch either area for progress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8b3870",
   "metadata": {},
   "source": [
    "### Cluster\n",
    "* for this exercise "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dddd8ea",
   "metadata": {},
   "source": [
    "What is a cluster?\n",
    "The notebooks are backed by clusters, or networked computers, that work together to process your data. The first step is to create a cluster. \n",
    "\n",
    "Create a cluster\n",
    "1. When your Azure Databricks workspace creation is complete, select the link to go to the resource.\n",
    "2. Select Launch Workspace to open your Databricks workspace in a new tab.\n",
    "3. In the left-hand menu of your Databricks workspace, select Clusters.\n",
    "4. Select Create Cluster to add a new cluster.\n",
    "5. Enter a name for your cluster. Use your name or initials to easily differentiate your cluster from those of your co-workers.\n",
    "6. Select the Cluster Mode: Single Node.\n",
    "7. Select the Databricks RuntimeVersion: Runtime: 7.3 LTS (Scala 2.12, Spark 3.0.1).\n",
    "8. Under Autopilot Options, leave the box checked and in the text box enter 45.\n",
    "9. Select the Node Type: F (with this free options you cannot afford anything else)\n",
    "10. Select Create Cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc78a82",
   "metadata": {},
   "source": [
    "## Table\n",
    "* uplodad de data to Azure Databricks using \"create -> table\"\n",
    "* the data ('pageviews_by_second.tsv') is here:\n",
    "https://mega.nz/folder/LAcGHJ4I#_uJ79tPCc4i5uWa0ps2itQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54f5db3",
   "metadata": {},
   "source": [
    "### Notebook\n",
    "* create new notebook and connect it to the cluster (top right corner)\n",
    "* start learning PySpark, try and test simple or more complex codes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b7daea",
   "metadata": {},
   "source": [
    "### Update you CV\n",
    "* basic knowledge on Spark (Pyspark) and Microsoft Azure Databricks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
