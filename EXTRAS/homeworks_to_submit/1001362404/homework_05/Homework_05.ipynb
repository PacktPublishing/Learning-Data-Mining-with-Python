{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "unsigned-intelligence",
   "metadata": {},
   "source": [
    "# Homework 05\n",
    "I. Take the airline opinion dataset (`airline_tweets.csv`) that was the subject of the Class 12  \n",
    "II. Divide the dataset into Train/Test: 80/20 percent fix the randomness!  \n",
    "III. Compare the different Methods:  \n",
    "\n",
    "  1. TextBlob\n",
    "  2. Microsoft Azure!!!\n",
    "  3. a train your own model with Sklearn algorithm, compare on the same subset!\n",
    "    * try different algorithms (https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html , play with parameters they offer) \n",
    "  3. * b try to train your model on completely new datasets LABELED (https://www.kaggle.com/datasets?search=sentiment&sort=votes&tags=13302-Classification&page=3)\n",
    "    * maybe improve your cleaning algorithm\n",
    "\n",
    "IV. Compare the results in the table  \n",
    "V. Write conclusions, \n",
    "   * how woud you detect sentences showing: hatred/racism/sexism, in Twitter (for example)\n",
    "   * which method would you choose to clasify opinions on Amazon products: positive/negative "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00edb688",
   "metadata": {},
   "source": [
    "The Follow work is done by:\n",
    "\n",
    "Joseph Nicolay Ruiz A., Physicist, University of Antioquia\n",
    "\n",
    "Dic, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "together-publisher",
   "metadata": {},
   "source": [
    "## Loading and cleaning of data \n",
    "Let's load the data `airline_tweets.csv` treated during class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "impressive-independence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_source_url = \"https://raw.githubusercontent.com/mhemmg/datasets/master/nlp/airline_tweets.csv\"\n",
    "airline_tweets = pd.read_csv(data_source_url)\n",
    "airline_tweets.to_csv(\"airline_tweets.csv\", index=False)\n",
    "airline_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85288702",
   "metadata": {},
   "source": [
    "Only it's necessary columns [`airline_sentiment`, `text`]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "9301881b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14452, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text\n",
       "0           neutral                @VirginAmerica What @dhepburn said.\n",
       "1          positive  @VirginAmerica plus you've added commercials t...\n",
       "2           neutral  @VirginAmerica I didn't today... Must mean I n...\n",
       "3          negative  @VirginAmerica it's really aggressive to blast...\n",
       "4          negative  @VirginAmerica and it's a really big bad thing..."
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airline_tweets1 = airline_tweets[['airline_sentiment','text']].copy()\n",
    "airline_tweets1.airline_sentiment.dropna(inplace=True)\n",
    "airline_tweets1.drop_duplicates(inplace=True, ignore_index=True)\n",
    "#airline_tweets1.to_csv('full.csv',index=False)\n",
    "print(airline_tweets1.shape)\n",
    "airline_tweets1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d9ba54",
   "metadata": {},
   "source": [
    "Now, we need to clean the `text`column, eliminating: user accounts, numeric and special character, stop words, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "fabb28d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy \n",
    "#from spacy.cli import download\n",
    "#print(download('en_core_web_sm'))\n",
    "from nlppreprocess import NLP\n",
    "nlp_ = NLP()\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "b81277cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(feature):    \n",
    "\n",
    "    # removing tweet\n",
    "    processed_feature = re.sub(r'@\\w+', ' ', str(feature))\n",
    "\n",
    "    # removing retweet\n",
    "    processed_feature = re.sub(r'rt @\\w+:', ' ', processed_feature)\n",
    "\n",
    "    # removing numeric\n",
    "    processed_feature = re.sub(\"\\d+\", ' ', processed_feature)\n",
    "\n",
    "    # Removing links\n",
    "    processed_feature = re.sub(r'http\\S+', ' ', processed_feature)\n",
    "\n",
    "    # Remove all the special characters except (') \n",
    "    processed_feature = re.sub(r\"[^\\w']\", ' ', processed_feature)\n",
    "\n",
    "    # Remove single characters\n",
    "    processed_feature = re.sub(r'^[a-zA-Z]\\s+|\\s+[a-zA-Z]\\s+|\\s+[a-zA-Z]$', ' ', processed_feature) \n",
    "\n",
    "    # Substituting multiple spaces with single space\n",
    "    processed_feature = re.sub(r'\\s+', ' ', processed_feature, flags=re.I)\n",
    "\n",
    "    # Converting to Lowercase\n",
    "    processed_feature = processed_feature.lower()\n",
    "\n",
    "    # Removing StopWords\n",
    "    #Note: It'snt conveniente remove stopwords as usual for sentiment analysis.\n",
    "    #processed_feature = \" \".join(token.lemma_ for token in nlp(processed_feature) if not token.is_stop)\n",
    "    processed_feature = nlp_.process(processed_feature) #This method is better\n",
    "    \n",
    "    return processed_feature    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "ea21fbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "collections = np.array_split(airline_tweets1.index.to_numpy(),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "20e43e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "def run_cleaning(collection, df=airline_tweets1):\n",
    "    data = [cleaner(tweet) for tweet in df.text[collection]]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "d9427f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(processes= mp.cpu_count()-1)\n",
    "    tasks = collections\n",
    "    results = pool.map(run_cleaning, tasks)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "airline_tweets1['text_processed'] = [tweet for result in results for tweet in result]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499b079f",
   "metadata": {},
   "source": [
    "It yields the following result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "033a58f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>text_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>what said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>plus you ve added commercials experience tacky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>not today must mean need take another trip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>it s really aggressive blast obnoxious enterta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>and it s really big bad thing about</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text  \\\n",
       "0           neutral                @VirginAmerica What @dhepburn said.   \n",
       "1          positive  @VirginAmerica plus you've added commercials t...   \n",
       "2           neutral  @VirginAmerica I didn't today... Must mean I n...   \n",
       "3          negative  @VirginAmerica it's really aggressive to blast...   \n",
       "4          negative  @VirginAmerica and it's a really big bad thing...   \n",
       "\n",
       "                                      text_processed  \n",
       "0                                          what said  \n",
       "1     plus you ve added commercials experience tacky  \n",
       "2         not today must mean need take another trip  \n",
       "3  it s really aggressive blast obnoxious enterta...  \n",
       "4                and it s really big bad thing about  "
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airline_tweets1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10eca25",
   "metadata": {},
   "source": [
    "Let's eliminate some irregularities ouputs such as 'empty' results after cleaning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "89a283f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask  = airline_tweets1.text_processed == ' '\n",
    "airline_tweets1 = airline_tweets1.drop(airline_tweets1[mask].index).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7be6660",
   "metadata": {},
   "source": [
    "Define the features and labels for problem since the last above data frame columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "fda5e936",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = airline_tweets1.iloc[:, 1].values\n",
    "features_processed = airline_tweets1.iloc[:, 2].values\n",
    "labels = airline_tweets1.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998371fe",
   "metadata": {},
   "source": [
    "## Application of models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fa354c",
   "metadata": {},
   "source": [
    "Let's star applying `TexBlob`. The results are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "cc8f582e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "59c0fcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_sentiment(tweet):    \n",
    "    \n",
    "    analysis = TextBlob(tweet)\n",
    "    \n",
    "    if analysis.sentiment.polarity > 0:\n",
    "        return 'positive'\n",
    "    elif analysis.sentiment.polarity == 0:\n",
    "        return 'neutral'\n",
    "    else:\n",
    "        return 'negative' \n",
    "    \n",
    "def Text_Blob(tweets):\n",
    "    return list(map(get_tweet_sentiment, tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "473958ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-processed: 0.46\n",
      "Processed: 0.46\n"
     ]
    }
   ],
   "source": [
    "TextBlob_Classifier = Text_Blob(features)\n",
    "print(f'Non-processed: {accuracy_score(labels, TextBlob_Classifier):.2f}')\n",
    "TextBlob_Classifier = Text_Blob(features_processed)\n",
    "print(f'Processed: {accuracy_score(labels, TextBlob_Classifier):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae3ad0f",
   "metadata": {},
   "source": [
    "Besides, using `Microsoft Azure` (Cloud computing):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "5c6641e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"c008ac7aa1244c1a9b469bd8282355af\" \n",
    "endpoint = \"https://jnra.cognitiveservices.azure.com/\" \n",
    "\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Authenticate the client using your key and endpoint \n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=endpoint, \n",
    "            credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "cliente = authenticate_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "0655ce30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis_azure(documents):\n",
    "    \n",
    "    result = client.analyze_sentiment(list(documents), show_opinion_mining=False)\n",
    "    doc_result = [doc.sentiment for doc in result if not doc.is_error]\n",
    "    \n",
    "    return doc_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50de919c",
   "metadata": {},
   "source": [
    "To avoid overpass the call volume quota for TextAnalytics F0 pricing tier, we're going to consider a sample smaller tha working-complete data set of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "3a1e5c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(21)\n",
    "sample = np.sort(random.sample(range(len(labels)), 400))\n",
    "\n",
    "labels_sample = labels[sample]\n",
    "features_sample = features_processed[sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "7ed94b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = np.array_split(features_sample, len(features_sample)/10)\n",
    "if __name__ == \"__main__\":\n",
    "    pool = mp.Pool(processes= mp.cpu_count()-1)\n",
    "    results = pool.map(sentiment_analysis_azure, list(tasks))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "azure_Classifier =[sentiment for result in results for sentiment in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "55d361b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: 0.60\n"
     ]
    }
   ],
   "source": [
    "print(f'Processed: {accuracy_score(labels_sample, azure_Classifier):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45da1623",
   "metadata": {},
   "source": [
    "Now, we can train our own model with Sklearn algorithm and compare.\n",
    "Firstly, vectorize the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "4aeb8aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(features_processed).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe31a48",
   "metadata": {},
   "source": [
    "Splitting of train, test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "1084081c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "219eed1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5951ac42",
   "metadata": {},
   "source": [
    "I'm going to implement with parameter set up by default. It's possible optimize using GridSearch from Sklearn "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88478aa",
   "metadata": {},
   "source": [
    "`RadomForestClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "44c7be81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7540643375994466\n"
     ]
    }
   ],
   "source": [
    "RF_Classifier = RandomForestClassifier()\n",
    "RF_Classifier.fit(X_train, y_train)\n",
    "predictions = RF_Classifier.predict(X_test)\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aae8eaa",
   "metadata": {},
   "source": [
    "`GradientBoostingClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceffd3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "GB_Classifier = GradientBoostingClassifier()\n",
    "GB_Classifier.fit(X_train, y_train)\n",
    "predictions = GB_Classifier.predict(X_test)\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2d6b11",
   "metadata": {},
   "source": [
    "`MLPClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5b24e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_Classifier = MLPClassifier()\n",
    "MLP_Classifier.fit(X_train, y_train)\n",
    "predictions = MLP_Classifier.predict(X_test)\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bd4fd8",
   "metadata": {},
   "source": [
    "`AdaBoostClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7969ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "AB_classifier = AdaBoostClassifier()\n",
    "AB_classifier.fit(X_train, y_train)\n",
    "predictions = AB_classifier.predict(X_test)\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8aae481",
   "metadata": {},
   "source": [
    "`LinearSVC`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "f9c3f926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7838118298166724\n"
     ]
    }
   ],
   "source": [
    "LSVC_Classifier = LinearSVC()\n",
    "LSVC_Classifier.fit(X_train, y_train)\n",
    "predictions = LSVC_Classifier.predict(X_test)\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e29735",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "For implement pre-training model is fundamental how we clean (process) the text to be examined. Remove directly all stopWords may conclude wrong results for sentiment analysis. Besides, although we didn't adjust the best parameters for train models, we can conclude the best, as in time as in perfomance is the model `LinearSVC`, with an accuracy close to 80%. \n",
    "\n",
    "The results yielded by Microsoft Azure conclude additional information as subjetivity and weight for each type of possible sentiment. With this last information we can classify a tweet (or any other comment) as hatred,racism,sexism, etc. For example:\n",
    "Let be ('negative'=x, 'positive'=y, 'neutral'=z), such a way there exist some unique relation among numerical weights (x,y,z) for each category (sexist,racist,...).\n",
    "\n",
    "Another option, it consists on implement HugginFace (Yes, another pre-trained model)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
